{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20203c0f",
   "metadata": {},
   "source": [
    "# I. Project Team Members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3d7f4",
   "metadata": {},
   "source": [
    "| Prepared by | Email | Prepared for |\n",
    "| :-: | :-: | :-: |\n",
    "| **_Your Name_** | _Your Email_ | **_Project Name_** |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b05cd469",
   "metadata": {},
   "source": [
    "# II. Notebook Target Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47bae1d2",
   "metadata": {},
   "source": [
    "_Insert Text Here_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3213f42d",
   "metadata": {},
   "source": [
    "# III. Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb5c3810",
   "metadata": {},
   "source": [
    "## III.A. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from hyperopt import fmin, tpe, space_eval, Trials, STATUS_OK\n",
    "from interpret import set_visualize_provider, show\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.provider import InlineProvider\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import hashlib\n",
    "import hyperopt\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "set_visualize_provider(InlineProvider())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7291e85b",
   "metadata": {},
   "source": [
    "## III.B. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f425995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../../data/processed/X_train_woe.pkl')\n",
    "X_test = pd.read_pickle('../../data/processed/X_test_woe.pkl')\n",
    "y_train = pd.read_pickle('../../data/processed/y_train.pkl')\n",
    "y_test = pd.read_pickle('../../data/processed/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3fa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f59e32c9",
   "metadata": {},
   "source": [
    "# IV. Models Training and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0a5756",
   "metadata": {},
   "source": [
    "## IV.A. Data Shape Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3389b1bf",
   "metadata": {},
   "source": [
    "## IV.B. Data Information Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe621948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce34b86f",
   "metadata": {},
   "source": [
    "## IV.C. Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stamp():\n",
    "    return datetime.now()\n",
    "\n",
    "\n",
    "def create_logger():\n",
    "    return {\n",
    "        \"model_name\": [],\n",
    "        \"model_uid\": [],\n",
    "        \"training_time\": [],\n",
    "        \"training_date\": [],\n",
    "        \"performance\": [],\n",
    "        \"f1_score_avg\": [],\n",
    "        \"auc_roc\": [],\n",
    "        \"gini\": [],\n",
    "        \"data_configurations\": []\n",
    "    }\n",
    "\n",
    "\n",
    "def training_log_updater(current_log, log_path):\n",
    "    try:\n",
    "        with open(log_path, 'r') as file:\n",
    "            last_log = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        with open(log_path, 'w') as file:\n",
    "            file.write(\"[]\")\n",
    "        with open(log_path, 'r') as file:\n",
    "            last_log = json.load(file)\n",
    "    last_log.append(current_log)\n",
    "    with open(log_path, 'w') as file:\n",
    "        json.dump(last_log, file)\n",
    "    return last_log\n",
    "\n",
    "\n",
    "def model_training_and_evaluation(model_factory, model_prefix, X_train, y_train, X_test, y_test, data_configuration, log_path):\n",
    "    def check_log_length(log_path):\n",
    "        try:\n",
    "            with open(log_path, 'r') as file:\n",
    "                logs = json.load(file)\n",
    "                return len(logs)\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "    before_training_len = check_log_length(log_path)\n",
    "    logger = create_logger()\n",
    "    current_training_models = []\n",
    "    for model in tqdm(model_factory()):\n",
    "        model_name = model_prefix + \"-\" + model[\"model_name\"]\n",
    "        start_time = time_stamp()\n",
    "        model[\"model_object\"].fit(X_train, y_train)\n",
    "        finished_time = time_stamp()\n",
    "        elapsed_time = (finished_time - start_time).total_seconds()\n",
    "        y_prediction = model[\"model_object\"].predict(X_test)\n",
    "        performance = classification_report(\n",
    "            y_test, y_prediction, output_dict=True)\n",
    "        y_probs = model[\"model_object\"].predict_proba(X_test)[:, 1]\n",
    "        auc_roc = roc_auc_score(y_test, y_probs)\n",
    "        gini = 2 * auc_roc - 1\n",
    "        original_id = str(start_time) + str(finished_time)\n",
    "        hashed_id = hashlib.md5(original_id.encode()).hexdigest()\n",
    "        model[\"model_uid\"] = hashed_id\n",
    "        logger[\"model_name\"].append(model_name)\n",
    "        logger[\"model_uid\"].append(hashed_id)\n",
    "        logger[\"training_time\"].append(elapsed_time)\n",
    "        logger[\"training_date\"].append(str(start_time))\n",
    "        logger[\"performance\"].append(performance)\n",
    "        logger[\"f1_score_avg\"].append(performance[\"macro avg\"][\"f1-score\"])\n",
    "        logger[\"auc_roc\"].append(auc_roc)\n",
    "        logger[\"gini\"].append(gini)\n",
    "        logger[\"data_configurations\"].append(data_configuration)\n",
    "        current_training_models.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"model_object\": deepcopy(model[\"model_object\"]),\n",
    "            \"model_uid\": model[\"model_uid\"]\n",
    "        })\n",
    "    training_log = training_log_updater(logger, log_path)\n",
    "    after_training_len = check_log_length(log_path)\n",
    "    print(f\"Logs Before Training: {before_training_len}\")\n",
    "    print(f\"Logs After Training: {after_training_len}\")\n",
    "    print(f\"Added {after_training_len - before_training_len} new logs.\")\n",
    "    return training_log, current_training_models\n",
    "\n",
    "\n",
    "def model_training_and_evaluation_skf(model_factory, model_prefix, X_train, y_train, X_test, y_test, data_configuration, log_path, n_splits=5, datasets_per_fold=None):\n",
    "    def check_log_length(log_path):\n",
    "        try:\n",
    "            with open(log_path, 'r') as file:\n",
    "                logs = json.load(file)\n",
    "                return len(logs)\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "    before_training_len = check_log_length(log_path)\n",
    "    logger = create_logger()\n",
    "    X = pd.concat([X_train, X_test], axis=0)\n",
    "    y = pd.concat([y_train, y_test], axis=0)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=777)\n",
    "    current_training_models = []\n",
    "    if datasets_per_fold is None:\n",
    "        datasets_per_fold = []\n",
    "    for model in tqdm(model_factory()):\n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "            model_name = f\"{model_prefix}-{model['model_name']}-fold_{fold+1}\"\n",
    "            start_time = time_stamp()\n",
    "            X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "            X_train_current, X_test_current = X_train_fold, X_val_fold\n",
    "            fold_data_dict = {\n",
    "                \"model_name\": model_name,\n",
    "                \"n_fold\": f\"fold_{fold+1}\",\n",
    "                \"X_train\": pd.DataFrame(X_train_fold),\n",
    "                \"y_train\": pd.Series(y_train_fold),\n",
    "                \"X_test\": pd.DataFrame(X_val_fold),\n",
    "                \"y_test\": pd.Series(y_val_fold)\n",
    "            }\n",
    "            datasets_per_fold.append(fold_data_dict)\n",
    "            model[\"model_object\"].fit(X_train_current, y_train_fold)\n",
    "            finished_time = time_stamp()\n",
    "            elapsed_time = (finished_time - start_time).total_seconds()\n",
    "            y_prediction = model[\"model_object\"].predict(X_test_current)\n",
    "            performance = classification_report(\n",
    "                y_val_fold, y_prediction, output_dict=True)\n",
    "            y_probs = model[\"model_object\"].predict_proba(X_test_current)[:, 1]\n",
    "            auc_roc = roc_auc_score(y_val_fold, y_probs)\n",
    "            gini = 2 * auc_roc - 1\n",
    "            original_id = str(start_time) + str(finished_time)\n",
    "            hashed_id = hashlib.md5(original_id.encode()).hexdigest()\n",
    "            model[\"model_uid\"] = hashed_id\n",
    "            logger[\"model_name\"].append(model_name)\n",
    "            logger[\"model_uid\"].append(hashed_id)\n",
    "            logger[\"training_time\"].append(elapsed_time)\n",
    "            logger[\"training_date\"].append(str(start_time))\n",
    "            logger[\"performance\"].append(performance)\n",
    "            logger[\"f1_score_avg\"].append(performance[\"macro avg\"][\"f1-score\"])\n",
    "            logger[\"auc_roc\"].append(auc_roc)\n",
    "            logger[\"gini\"].append(gini)\n",
    "            logger[\"data_configurations\"].append(data_configuration)\n",
    "            current_training_models.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"model_object\": deepcopy(model[\"model_object\"]),\n",
    "                \"model_uid\": model[\"model_uid\"]\n",
    "            })\n",
    "    training_log = training_log_updater(logger, log_path)\n",
    "    after_training_len = check_log_length(log_path)\n",
    "    print(f\"Logs Before Training: {before_training_len}\")\n",
    "    print(f\"Logs After Training: {after_training_len}\")\n",
    "    print(f\"Added {after_training_len - before_training_len} new logs.\")\n",
    "    return training_log, current_training_models, datasets_per_fold\n",
    "\n",
    "\n",
    "def training_log_to_df_converter(training_log):\n",
    "    all_training_logs_df = pd.DataFrame()\n",
    "    for log in tqdm(training_log):\n",
    "        individual_log_df = pd.DataFrame(log)\n",
    "        performance_df = pd.json_normalize(individual_log_df[\"performance\"])\n",
    "        individual_log_df = pd.concat([individual_log_df.drop(\n",
    "            \"performance\", axis=1), performance_df], axis=1)\n",
    "        all_training_logs_df = pd.concat(\n",
    "            [all_training_logs_df, individual_log_df])\n",
    "    all_training_logs_df.sort_values([\"f1_score_avg\", \"auc_roc\", \"training_time\"], ascending=[\n",
    "                                     False, False, True], inplace=True)\n",
    "    all_training_logs_df.reset_index(inplace=True, drop=True)\n",
    "    return all_training_logs_df\n",
    "\n",
    "\n",
    "def best_model_finder(all_training_logs_df, models_list):\n",
    "    model_object = None\n",
    "    best_model_info = all_training_logs_df.iloc[0]\n",
    "    for configuration_data in models_list:\n",
    "        for model_data in models_list[configuration_data]:\n",
    "            if model_data[\"model_uid\"] == best_model_info[\"model_uid\"]:\n",
    "                model_object = model_data[\"model_object\"]\n",
    "                break\n",
    "    if model_object == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    return model_object\n",
    "\n",
    "def tuned_model_finder(models_list_tuned, tuning_method):\n",
    "    for model in models_list_tuned:\n",
    "        if tuning_method in model[\"model_name\"]:\n",
    "            return model[\"model_object\"]\n",
    "    print(f\"No model found that was tuned with {tuning_method}\")\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69eefd12",
   "metadata": {},
   "source": [
    "## IV.D. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(prefix):\n",
    "    return [\n",
    "        {\"model_name\": prefix + \"_DecisionTreeClassifier\",\n",
    "            \"model_object\": DecisionTreeClassifier(random_state=777), \"model_uid\": \"\"},\n",
    "        {\"model_name\": prefix + \"_ExplainableBoostingClassifier\",\n",
    "            \"model_object\": ExplainableBoostingClassifier(random_state=777), \"model_uid\": \"\"},\n",
    "        {\"model_name\": prefix + \"_LogisticRegression\",\n",
    "            \"model_object\": LogisticRegression(random_state=777), \"model_uid\": \"\"},\n",
    "        {\"model_name\": prefix + \"_RandomForestClassifier\",\n",
    "            \"model_object\": RandomForestClassifier(random_state=777), \"model_uid\": \"\"},\n",
    "        {\"model_name\": prefix + \"_XGBClassifier\",\n",
    "            \"model_object\": XGBClassifier(random_state=777), \"model_uid\": \"\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_factory = {\n",
    "    \"vanilla\": lambda: create_models(\"vanilla\"),\n",
    "    \"sampling\": lambda: create_models(\"sampling\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fde862-2df9-475f-a2c6-78bbba00227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_factory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a98cf1f",
   "metadata": {},
   "source": [
    "### IV.D.1. Vanilla Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853db323",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_vanilla = model_training_and_evaluation(\n",
    "    models_factory[\"vanilla\"],\n",
    "    \"vanilla\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"vanilla\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9befe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_vanilla, datasets_per_fold_vanilla = model_training_and_evaluation_skf(\n",
    "    models_factory[\"vanilla\"],\n",
    "    \"vanilla\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"vanilla\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a23e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list_vanilla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03126b44",
   "metadata": {},
   "source": [
    "### IV.D.2. Sampling Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_sampling = model_training_and_evaluation(\n",
    "    models_factory[\"sampling\"],\n",
    "    \"sampling\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"sampling\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_sampling, datasets_per_fold = model_training_and_evaluation_skf(\n",
    "    models_factory[\"sampling\"],\n",
    "    \"sampling\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"sampling\",\n",
    "    '../../models/logs/training_log.json',\n",
    "    datasets_per_fold=datasets_per_fold_vanilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9eb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = {\n",
    "    \"vanilla\": models_list_vanilla,\n",
    "    \"sampling\": models_list_sampling\n",
    "}\n",
    "models_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ac844f9",
   "metadata": {},
   "source": [
    "## IV.E. Models Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d259e2f-1538-4548-9e7f-5f4e9871be78",
   "metadata": {},
   "source": [
    "### IV.E.1. Benchmark Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance that a model would achieve if it always predicted the most common label.\n",
    "benchmark = y_train.value_counts(normalize=True)[0]\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f14d9-88d7-4ba6-b116-929db1b76c89",
   "metadata": {},
   "source": [
    "### IV.E.2. Baseline Base Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2eaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_logs_df = training_log_to_df_converter(training_log)\n",
    "all_training_logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3425653",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_logs_df.to_csv('../../reports/baseline_model.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_best_model = best_model_finder(all_training_logs_df, models_list)\n",
    "baseline_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_info = all_training_logs_df.iloc[0]\n",
    "print(\"Best model configuration:\", best_model_info[\"data_configurations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c40f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_algorithm(all_training_logs_df, algorithm_name):\n",
    "    filtered_df = all_training_logs_df[all_training_logs_df[\"model_name\"].str.contains(\n",
    "        algorithm_name)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def average_performance_metrics(filtered_df, f1_score_column=\"f1_score_avg\", roc_auc_column=\"auc_roc\"):\n",
    "    avg_f1_score = filtered_df[f1_score_column].mean()\n",
    "    avg_roc_auc = filtered_df[roc_auc_column].mean()\n",
    "    return avg_f1_score, avg_roc_auc\n",
    "\n",
    "\n",
    "def get_best_model_and_dataset(model_name, models_list, datasets_per_fold):\n",
    "    model_instance = None\n",
    "    model_data = None\n",
    "    for key in models_list:\n",
    "        for model_info in models_list[key]:\n",
    "            if model_info[\"model_name\"] == model_name:\n",
    "                model_instance = model_info[\"model_object\"]\n",
    "                break\n",
    "        if model_instance is not None:\n",
    "            break\n",
    "    for data in datasets_per_fold:\n",
    "        if data[\"model_name\"] == model_name:\n",
    "            model_data = data\n",
    "            break\n",
    "    return model_instance, model_data\n",
    "\n",
    "\n",
    "def get_metrics_dataframe(model, X_train, y_train, X_test, y_test):\n",
    "    train_prediction = model.predict(X_train)\n",
    "    test_prediction = model.predict(X_test)\n",
    "    train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def get_prediction_metrics(y_true, y_pred, y_probs):\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_probs)\n",
    "        gini = 2 * auc_roc - 1\n",
    "        metrics = {\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"auc_roc\": auc_roc,\n",
    "            \"gini\": gini\n",
    "        }\n",
    "        return metrics\n",
    "    train_metrics = get_prediction_metrics(\n",
    "        y_train, train_prediction, train_probs)\n",
    "    train_metrics[\"dataset\"] = \"Train\"\n",
    "    test_metrics = get_prediction_metrics(y_test, test_prediction, test_probs)\n",
    "    test_metrics[\"dataset\"] = \"Test\"\n",
    "    return pd.DataFrame([train_metrics, test_metrics])\n",
    "\n",
    "\n",
    "def display_confusion_matrix(model, X_train, y_train, X_test, y_test):\n",
    "    train_prediction = model.predict(X_train)\n",
    "    test_prediction = model.predict(X_test)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_train, train_prediction, ax=ax[0])\n",
    "    ax[0].set_title(\"Train Confusion Matrix\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, test_prediction, ax=ax[1])\n",
    "    ax[1].set_title(\"Test Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train_vs_test_error(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    train_error = 1 - accuracy_score(y_train, y_pred_train)\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred_test)\n",
    "    bars = plt.bar([\"Train Error\", \"Test Error\"], [train_error, test_error])\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Train vs Test Error\")\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005,\n",
    "                 round(yval, 2), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_prob)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fpr_train, tpr_train,\n",
    "             label=f\"Train AUC: {roc_auc_score(y_train, y_pred_train_prob):.2f}\")\n",
    "    plt.plot(fpr_test, tpr_test,\n",
    "             label=f\"Test AUC: {roc_auc_score(y_test, y_pred_test_prob):.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_learning_curve(model, X, y, cv=50):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), n_jobs=-1)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-',\n",
    "             color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-',\n",
    "             color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = \"Algorithm\"\n",
    "algo_baseline_df = filter_by_algorithm(all_training_logs_df, algorithm_name)\n",
    "algo_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8654ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_avg_f1_score, algo_avg_auc_roc = average_performance_metrics(\n",
    "    algo_baseline_df)\n",
    "print(f\"Average F1 Score for {algorithm_name}: {algo_avg_f1_score:.4f}\")\n",
    "print(f\"Average AUC-ROC for {algorithm_name}: {algo_avg_auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755af092",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_best_model_name = algo_baseline_df.iloc[0][\"model_name\"]\n",
    "algo_best_model, algo_best_model_data = get_best_model_and_dataset(\n",
    "    algo_best_model_name, models_list, datasets_per_fold)\n",
    "algo_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_algo_best_baseline = algo_best_model_data[\"X_train\"]\n",
    "X_test_algo_best_baseline = algo_best_model_data[\"X_test\"]\n",
    "y_train_algo_best_baseline = algo_best_model_data[\"y_train\"]\n",
    "y_test_algo_best_baseline = algo_best_model_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0718cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9116f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(baseline_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(baseline_best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_baseline_df.to_csv('../../reports/algo_baseline_model.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bb2fc6d",
   "metadata": {},
   "source": [
    "### IV.E.3. Export Baseline Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../models/baseline_best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(baseline_best_model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de0ed9a4",
   "metadata": {},
   "source": [
    "## IV.F. Hyperparameters Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be189d67",
   "metadata": {},
   "source": [
    "### IV.F.1. Hyperparameters List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882ba16",
   "metadata": {},
   "source": [
    "#### IV.F.1.A. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7713f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_hyperparams = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5663b2-0c93-4dfb-b23d-79e98eac1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=777),\n",
    "    log_reg_hyperparams,\n",
    "    n_jobs=-1,\n",
    "    verbose=420,\n",
    "    scoring='f1_macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc81ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_from_grid = log_reg_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30add73-650f-41b2-848a-9f5e56568dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list[\"fine-tuned\"] = [{\"model_name\": \"GridSearchBest-LogisticRegression\",\n",
    "                              \"model_object\": best_estimator_from_grid, \"model_uid\": \"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aaa7f",
   "metadata": {},
   "source": [
    "#### IV.F.1.B. Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_space = {\n",
    "    'penalty': hyperopt.hp.choice('penalty', ['l1', 'l2']),\n",
    "    'C': hyperopt.hp.loguniform('C', np.log(1e-4), np.log(1e4)),\n",
    "    'solver': 'liblinear'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    classifier = LogisticRegression(**params, random_state=777)\n",
    "    score = cross_val_score(classifier, X_train,\n",
    "                            y_train, cv=5, scoring='f1_macro').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=log_reg_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "best_params = space_eval(log_reg_space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_log_reg = LogisticRegression(**best_params, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list[\"fine-tuned\"].append({\"model_name\": \"BayesOpt-LogisticRegression\",\n",
    "                                  \"model_object\": optimal_log_reg, \"model_uid\": \"\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7943a5bb",
   "metadata": {},
   "source": [
    "### IV.F.2. Best Model Hyperparameter Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_tuned = model_training_and_evaluation(\n",
    "    models_list[\"fine-tuned\"],\n",
    "    \"tuned_model\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"tuned\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c59ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc10a9-fdee-4c0b-951a-5ff793172a27",
   "metadata": {},
   "source": [
    "### IV.F.3. Hyperparameter-tuned Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f99ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_logs_df_tuned = training_log_to_df_converter(training_log)\n",
    "all_training_logs_df_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd33386",
   "metadata": {},
   "source": [
    "#### IV.F.3.A. Grid Searched Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_tuned = {\"fine-tuned\": models_list_tuned}\n",
    "tuned_best_model = tuned_model_finder(\n",
    "    models_dict_tuned[\"fine-tuned\"], \"GridSearchBest\")\n",
    "tuned_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab61f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38be23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56320b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(tuned_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e821a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(tuned_best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c807ae",
   "metadata": {},
   "source": [
    "#### IV.F.3.B. Bayesian Searched Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_tuned = {\"fine-tuned\": models_list_tuned}\n",
    "tuned_best_model = tuned_model_finder(\n",
    "    models_dict_tuned[\"fine-tuned\"], \"BayesOpt\")\n",
    "tuned_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(tuned_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(tuned_best_model, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c44a6b",
   "metadata": {},
   "source": [
    "### IV.F.4. Export Hyperparameter-tuned Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../models/tuned_best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(tuned_best_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
